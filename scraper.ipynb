{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://codelibrary.amlegal.com/codes/san_francisco/latest/sf_police\"\n",
    "        \n",
    "# Setup Chrome driver\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "wait = WebDriverWait(driver, 10)\n",
    "driver.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Municipal Code table of contents...\n",
      "Found 0 chapters\n",
      "Saved 0 chapters to sf_municipal_code.json\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SFMunicipalCodeScraper:\n",
    "    def __init__(self, headless=False):\n",
    "        self.base_url = \"https://codelibrary.amlegal.com/codes/san_francisco/latest/sf_police\"\n",
    "        \n",
    "        # Setup Chrome driver\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        \n",
    "    def get_table_of_contents(self):\n",
    "        \"\"\"Get the main table of contents with all chapters\"\"\"\n",
    "        print(\"Loading Municipal Code table of contents...\")\n",
    "        \n",
    "        self.driver.get(self.base_url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        \n",
    "        # Find all chapter links\n",
    "        chapter_links = []\n",
    "        \n",
    "        # Look for links that contain chapter information\n",
    "        links = self.driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            text = link.text.strip()\n",
    "            \n",
    "            # Filter for actual chapter links\n",
    "            if href and \"chapter\" in href.lower() and text:\n",
    "                chapter_links.append({\n",
    "                    'title': text,\n",
    "                    'url': href,\n",
    "                    'chapter_number': self.extract_chapter_number(text)\n",
    "                })\n",
    "        \n",
    "        print(f\"Found {len(chapter_links)} chapters\")\n",
    "        return chapter_links\n",
    "    \n",
    "    def extract_chapter_number(self, text):\n",
    "        \"\"\"Extract chapter number from text\"\"\"\n",
    "        match = re.search(r'chapter\\s*(\\d+)', text, re.IGNORECASE)\n",
    "        return match.group(1) if match else None\n",
    "    \n",
    "    def scrape_chapter(self, chapter_info):\n",
    "        \"\"\"Scrape content of a specific chapter\"\"\"\n",
    "        print(f\"Scraping chapter: {chapter_info['title']}\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(chapter_info['url'])\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Wait for content to load\n",
    "            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Extract the main content\n",
    "            content_div = soup.find('div', class_='main-content') or soup.find('div', id='content')\n",
    "            \n",
    "            if not content_div:\n",
    "                # Fallback: get all text content\n",
    "                content_div = soup.find('body')\n",
    "            \n",
    "            # Clean and extract text\n",
    "            chapter_text = self.clean_municipal_code_text(content_div.get_text())\n",
    "            \n",
    "            # Extract sections within the chapter\n",
    "            sections = self.extract_sections(soup)\n",
    "            \n",
    "            return {\n",
    "                'chapter_number': chapter_info['chapter_number'],\n",
    "                'title': chapter_info['title'],\n",
    "                'url': chapter_info['url'],\n",
    "                'content': chapter_text,\n",
    "                'sections': sections,\n",
    "                'scraped_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping chapter {chapter_info['title']}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_sections(self, soup):\n",
    "        \"\"\"Extract individual sections from a chapter\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Look for section headers (various possible formats)\n",
    "        section_patterns = [\n",
    "            re.compile(r'Sec\\.\\s*(\\d+\\.\\d+)', re.IGNORECASE),\n",
    "            re.compile(r'Section\\s*(\\d+\\.\\d+)', re.IGNORECASE),\n",
    "            re.compile(r'(\\d+\\.\\d+)\\s*\\.', re.IGNORECASE)\n",
    "        ]\n",
    "        \n",
    "        # Find all potential section headers\n",
    "        all_text = soup.get_text()\n",
    "        \n",
    "        for pattern in section_patterns:\n",
    "            matches = pattern.finditer(all_text)\n",
    "            for match in matches:\n",
    "                section_number = match.group(1)\n",
    "                # Extract section content (simplified approach)\n",
    "                sections.append({\n",
    "                    'section_number': section_number,\n",
    "                    'start_position': match.start()\n",
    "                })\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def clean_municipal_code_text(self, text):\n",
    "        \"\"\"Clean municipal code text\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove navigation elements\n",
    "        text = re.sub(r'Print\\s*View|Table of Contents|Search', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove copyright notices\n",
    "        text = re.sub(r'Â©.*?All rights reserved\\.?', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Clean up formatting\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def scrape_all_chapters(self, max_chapters=None):\n",
    "        \"\"\"Scrape all chapters of the municipal code\"\"\"\n",
    "        \n",
    "        # Get table of contents\n",
    "        chapters = self.get_table_of_contents()\n",
    "        \n",
    "        if max_chapters:\n",
    "            chapters = chapters[:max_chapters]\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        for i, chapter in enumerate(chapters):\n",
    "            print(f\"Processing chapter {i+1}/{len(chapters)}: {chapter['title']}\")\n",
    "            \n",
    "            chapter_data = self.scrape_chapter(chapter)\n",
    "            if chapter_data:\n",
    "                all_data.append(chapter_data)\n",
    "            \n",
    "            # Be respectful - add delay between requests\n",
    "            time.sleep(2)\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def save_data(self, data, filename=\"sf_municipal_code.json\"):\n",
    "        \"\"\"Save scraped data to JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved {len(data)} chapters to {filename}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the browser driver\"\"\"\n",
    "        self.driver.quit()\n",
    "\n",
    "# Usage example\n",
    "def scrape_sf_municipal_code():\n",
    "    scraper = SFMunicipalCodeScraper(headless=True)\n",
    "    \n",
    "    try:\n",
    "        # Scrape first 5 chapters for testing\n",
    "        data = scraper.scrape_all_chapters(max_chapters=5)\n",
    "        \n",
    "        # Save the data\n",
    "        scraper.save_data(data)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    municipal_code_data = scrape_sf_municipal_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "from pathlib import Path\n",
    "pdf_path = \"san_francisco-ca-1.pdf\"\n",
    "doc = fitz.open(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for page in doc:\n",
    "    text += page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "text = re.sub(r' {2,}', ' ', text)\n",
    "text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "articles = re.split(r'\\n\\s*Article\\s+([IVX]+):\\s*([A-Z\\sâ--]+)', text)\n",
    "chunks = []\n",
    "for i in range(1, len(articles), 3):\n",
    "    if i + 2 < len(articles):\n",
    "        article_num = articles[i]\n",
    "        article_title = articles[i + 1]\n",
    "        article_content = articles[i + 2]\n",
    "        sections = re.split(r'\\n\\s*Section\\s+(\\d+\\.\\d+):\\s*([A-Z\\s]+)', article_content)\n",
    "\n",
    "        for j in range(1, len(sections), 3):\n",
    "            if j + 2 < len(sections):\n",
    "                section_num = sections[j]\n",
    "                section_title = sections[j + 1]\n",
    "                section_content = sections[j + 2]\n",
    "\n",
    "                chunk = f\"SF Municipal Code Article {article_num}: {article_title.strip()}\\n\\n\"\n",
    "                chunk += f\"Section {section_num}: {section_title.strip()}\\n\\n\"\n",
    "                chunk += section_content.strip()\n",
    "\n",
    "                if chunk and len(tokenizer.encode(chunk)) < 250:\n",
    "                    chunks.append(chunk)\n",
    "                elif chunk:\n",
    "                    sub_chunks = split_long_section()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
